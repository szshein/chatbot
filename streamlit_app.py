import streamlit as st
import pandas as pd
import jieba
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
from matplotlib import font_manager
from PIL import Image

# --- ä¸­æ–‡å­—å‹è¨­å®š ---
font_path = "msyh.ttc"
font_prop = font_manager.FontProperties(fname=font_path)
plt.rcParams['font.family'] = font_prop.get_name()
plt.rcParams['axes.unicode_minus'] = False

# --- è¼‰å…¥èˆ‡å‰è™•ç†è³‡æ–™ ---
df = pd.read_csv("104_jobs_all.csv")
df = df.dropna(subset=['jobName', 'description', 'jobAddrNoDesc'])

def extract_content_and_qualification(text):
    pattern_content = re.compile(r'(?:å…§å®¹|è·è²¬|do(?:[ã€‘â€¢])*)(.*?)(?=(?:è³‡æ ¼|å¾…é‡|å ±å|æœŸé–“|æ¢ä»¶|$))', re.DOTALL)
    match_content = pattern_content.search(text)
    jobContent = match_content.group(1).strip() if match_content else text.strip()

    pattern_qualification = re.compile(r'(?:æ¢ä»¶|è³‡æ ¼)[ï¼š:](.*?)(?=(?:å·¥ä½œåœ°é»|å¾…é‡|æ¢ä»¶|è¦æ±‚|$))', re.DOTALL)
    match_qualification = pattern_qualification.search(text)
    jobQualification = match_qualification.group(1).strip() if match_qualification else ""

    return pd.Series([jobContent, jobQualification], index=["jobContent", "jobQualification"])

def extract_city(text):
    pattern_city = re.compile(r'^([^å¸‚ç¸£]+)(?=[å¸‚ç¸£])', re.DOTALL)
    match_city = pattern_city.search(text)
    return match_city.group(1).strip() if match_city else ""

df_content = df['description'].apply(extract_content_and_qualification)
df_city = df['jobAddrNoDesc'].apply(extract_city)
df_new = pd.concat([df[['jobName']], df_content, df_city.rename("jobCity")], axis=1)

# --- ä¸­æ–‡æ–·è©èˆ‡è©é »åˆ†æ ---
text_data = " ".join(df_new['jobContent'].dropna().tolist())
jieba.setLogLevel(20)
stopwords = set(["æˆ‘å€‘", "and", "ç›¸é—œ", "å…¬å¸", "æä¾›", "\r\n", "ç†Ÿæ‚‰", "å‡é·", "00", "to", "çš„", "æ˜¯", "åœ¨", "æœ‰", "ç‚º", "å’Œ", "ç‚ºäº†", 
                 "é‚£", "æœ‰äº›", "ä¸­", "é€™", "æ­¤", "ä¸Š", "ä¸‹", "èˆ‡", "æœ‰é—œ", "å¿…é ˆ", "æœƒ", "ä¹‹", "è€Œ", "æ‡‰", "ä¸€", "ä¹Ÿ", "å„", "æ‰€", "å³",
                 "ç­‰", "ç­‰æ–¼", "æˆ–è€…", "åš", "å¯ä»¥", "æ‡‰è©²", "æœƒ", "æƒ³", "ä¾†", "å»", "å—", "æ˜¯çš„", "å°", "æ²’æœ‰", "the"])
words = [w for w in jieba.cut(text_data) if len(w) > 1 and w not in stopwords]
word_counts = Counter(words)

# --- TF-IDF & LDA ---
def jieba_tokenizer(text):
    return [word for word in jieba.cut(text) if len(word) > 1 and word not in stopwords]

documents = df_new['jobContent'].dropna().tolist()
vectorizer_lda = CountVectorizer(tokenizer=jieba_tokenizer)
X = vectorizer_lda.fit_transform(documents)
lda = LatentDirichletAllocation(n_components=3, max_iter=30, random_state=0)
lda.fit(X)

# --- Streamlit UI è¨­å®š ---
st.set_page_config(page_title="æ‰¾å¯¦ç¿’ Chatbot", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Job Chatbot å¯¦ç¿’åŠ©ç†")

# --- åœ–ç‰‡ä¸Šå‚³åŠŸèƒ½ ---
st.markdown("### ğŸ–¼ï¸ ä¸Šå‚³ä¸€å¼µåœ–ç‰‡")
uploaded_file = st.file_uploader("é¸æ“‡ä¸€å¼µåœ–ç‰‡", type=["png", "jpg", "jpeg"])
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption="ä½ ä¸Šå‚³çš„åœ–ç‰‡", use_column_width=True)

# --- å°è©±æç¤º ---
st.markdown("""
æ­¡è¿ä½¿ç”¨æ‰¾å¯¦ç¿’ Chatbotï¼

**ä½ å¯ä»¥é€™æ¨£å•æˆ‘ï¼š**
- æ‰¾è³‡æ–™åˆ†æç›¸é—œçš„å¯¦ç¿’
- é¡¯ç¤ºç†±é–€è©å½™
- å“ªå€‹åŸå¸‚å¯¦ç¿’æœ€å¤šï¼Ÿ
- é¡¯ç¤ºä¸»é¡Œåˆ†æçµæœ
- ç•«å‡ºåŸå¸‚è·ç¼ºæŸ±ç‹€åœ–
""")

# --- å„²å­˜èŠå¤©è¨Šæ¯ç‹€æ…‹ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- å›æ‡‰ç”Ÿæˆé‚è¼¯ ---
def generate_response(prompt):
    prompt = prompt.lower()
    keyword_match = re.search(r'æ‰¾(.*?)(?:ç›¸é—œ)?çš„?(å¯¦ç¿’)?', prompt)
    if keyword_match:
        keyword = keyword_match.group(1).strip()
        is_intern = keyword_match.group(2) is not None
        return find_job_by_keyword(keyword, is_intern)
    elif "ç†±é–€" in prompt or "è©å½™" in prompt:
        return get_top_keywords()
    elif "ä¸»é¡Œ" in prompt:
        return show_topics()
    elif "ç•«å‡º" in prompt and "åŸå¸‚" in prompt:
        show_top_city_bar()
        return "æ­¤åœ–æ˜¯å‰ 10 åŸå¸‚çš„è·ç¼ºåˆ†å¸ƒåœ–è¡¨ã€‚"
    elif "å“ªå€‹åŸå¸‚" in prompt or "æœ€å¤šè·ç¼º" in prompt:
        return city_distribution()
    else:
        return "æˆ‘å¯ä»¥å¹«ä½ æŸ¥è©¢å¯¦ç¿’è·ç¼ºã€ç†±é–€è©å½™ã€ä¸»é¡Œæˆ–åŸå¸‚åˆ†å¸ƒï¼Œè«‹è©¦è‘—è¼¸å…¥ã€Œæ‰¾è³‡æ–™åˆ†æç›¸é—œçš„å¯¦ç¿’ã€é€™é¡å•é¡Œ"

def find_job_by_keyword(keyword, is_intern=False):
    # æœå°‹è·å‹™å…§å®¹æ¬„ä½ä¸­åŒ…å«é—œéµå­—çš„è·ç¼º
    df_filtered = df_new[
        df_new['jobContent'].str.contains(keyword, na=False, case=False)
    ]
    if is_intern:
        df_filtered = df_filtered[
            df_filtered['jobName'].str.contains("å¯¦ç¿’", na=False) |
            df_filtered['jobContent'].str.contains("å¯¦ç¿’", na=False)
        ]
    jobs = df_filtered['jobName'].head(5).tolist()
    if jobs:
        return f"èˆ‡ã€Œ{keyword}ã€ç›¸é—œçš„å‰ 5 ç­†{'å¯¦ç¿’' if is_intern else 'å·¥ä½œ'}ï¼š\n" + "\n".join(jobs)
    else:
        return f"æ‰¾ä¸åˆ°èˆ‡ã€Œ{keyword}ã€ç›¸é—œçš„è·ç¼ºï¼Œè«‹è©¦è©¦å…¶ä»–é—œéµå­—ã€‚"

def get_top_keywords():
    top_words = word_counts.most_common(10)
    return "ç†±é–€è©å½™ï¼š\n" + "\n".join([f"{w}ï¼ˆ{c}æ¬¡ï¼‰" for w, c in top_words])

def show_topics():
    result = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words = [vectorizer_lda.get_feature_names_out()[i] for i in topic.argsort()[:-6:-1]]
        result.append(f"ä¸»é¡Œ {topic_idx + 1}ï¼š{'ã€'.join(top_words)}")
    return "\n".join(result)

def city_distribution():
    city_counts = df_new['jobCity'].value_counts().head(5)
    return "æœ€å¤šè·ç¼ºçš„åŸå¸‚ï¼š\n" + "\n".join([f"{city}ï¼š{count}ç­†" for city, count in city_counts.items()])

def show_top_city_bar():
    city_counts = df_new['jobCity'].value_counts().head(10)
    fig, ax = plt.subplots()
    ax.bar(city_counts.index, city_counts.values, color="#6FA8DC")
    ax.set_title("å‰ 10 åŸå¸‚è·ç¼ºæ•¸é‡", fontproperties=font_prop)
    ax.set_xlabel("åŸå¸‚", fontproperties=font_prop)
    ax.set_ylabel("è·ç¼ºæ•¸", fontproperties=font_prop)
    plt.xticks(rotation=45, fontproperties=font_prop)
    plt.yticks(fontproperties=font_prop)
    st.pyplot(fig)

# --- è™•ç†ä½¿ç”¨è€…è¼¸å…¥ ---
if prompt := st.chat_input("è«‹å•ä½ æƒ³çŸ¥é“ä»€éº¼è·ç¼ºè³‡è¨Šï¼Ÿ", key="chat_bot"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    response = generate_response(prompt)
    st.chat_message("assistant").markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
